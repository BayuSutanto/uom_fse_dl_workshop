{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figs/se05.png)\n",
    "# Workshop Instructions\n",
    "***\n",
    "- <img src=\"figs/icons/write.svg\" width=\"20\" style=\"filter: invert(41%) sepia(96%) saturate(1449%) hue-rotate(210deg) brightness(100%) contrast(92%);\"/> Follow along by typing the code yourself - this helps with learning!\n",
    "- <img src=\"figs/icons/code.svg\" width=\"20\" style=\"filter: invert(100%) sepia(100%) saturate(2000%) hue-rotate(40deg) brightness(915%) contrast(100%);\"/> Code cells marked as \"Exercise\" are for you to complete\n",
    "- <img src=\"figs/icons/reminder.svg\" width=\"20\" style=\"filter: invert(100%) sepia(1500%) saturate(1500%) hue-rotate(30deg) brightness(450%) contrast(70%);\"/> Look for hints if you get stuck\n",
    "- <img src=\"figs/icons/success.svg\" width=\"20\" style=\"filter: invert(56%) sepia(71%) saturate(5293%) hue-rotate(117deg) brightness(95%) contrast(101%);\"/> Compare your solution with the provided answers\n",
    "- <img src=\"figs/icons/list.svg\" width=\"20\" style=\"filter: invert(19%) sepia(75%) saturate(6158%) hue-rotate(312deg) brightness(87%) contrast(116%);\"/> Don't worry if you make mistakes - debugging is part of learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "helper_utils = Path.cwd() / \"utils\"\n",
    "sys.path.append(str(helper_utils))\n",
    "\n",
    "import utils\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(101)\n",
    "torch.cuda.manual_seed(101)\n",
    "random.seed(101)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "checker = utils.core.ExerciseChecker(\"SE05\")\n",
    "quizzer = utils.core.QuizManager(\"SE05\")\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction to Transfer Learning\n",
    "***\n",
    "> <img src=\"figs/icons/write.svg\" width=\"20\" style=\"filter: invert(41%) sepia(96%) saturate(1449%) hue-rotate(210deg) brightness(100%) contrast(92%);\"/> **Definition**: Transfer learning is a machine learning technique where a model developed for one task is reused as a starting point for a model on a second task. It's particularly effective for deep learning models, as it allows us to leverage pre-trained models' knowledge rather than starting from scratch.\n",
    "\n",
    "In previous sessions, we learned how to build and train neural networks from scratch. However, training large deep learning models requires:\n",
    "\n",
    "1. **Massive datasets** (often millions of examples)\n",
    "2. **Extensive computational resources** (often multiple GPUs)\n",
    "3. **Long training times** (days to weeks)\n",
    "\n",
    "Transfer learning addresses these challenges by letting us capitalise on existing models that have already been trained on large datasets.\n",
    "\n",
    "<figure style=\"background-color: white; border-radius: 10px; padding: 20px; text-align: center; margin: 0 auto;\">\n",
    "    <!-- Suggestion: Create and add an image showing transfer learning concept with knowledge flowing from source to target domain -->\n",
    "    <img src=\"figs/transfer.png\" alt=\"Transfer Learning Concept\" style=\"width: 50%; height: auto; margin: 0 auto;\">\n",
    "</figure>\n",
    "\n",
    "Transfer learning is inspired by human learning. Consider how we learn:\n",
    "\n",
    "| Human Learning | Machine Learning Parallel |\n",
    "|----------------|--------------------------|\n",
    "| A child learns to recognize basic shapes before identifying letters | A model learns edge detection before specific object recognition |\n",
    "| A musician who knows piano can learn guitar faster than a novice | A model trained on one image dataset can adapt quickly to a similar task |\n",
    "| Language skills transfer across related languages (e.g., Spanish to Italian) | NLP models pre-trained on one language can be fine-tuned for another |\n",
    "\n",
    "This mirrors how neural networks learn hierarchical features. Early layers learn general patterns that are often applicable across domains, while later layers learn task-specific features.\n",
    "\n",
    "Transfer learning is particularly effective in computer vision and natural language processing (NLP) tasks, where large pre-trained models are available. The key advntages of transfer learning include:\n",
    "\n",
    "| Advantage | Description |\n",
    "|-----------|-------------|\n",
    "| **Reduced Training Time** | Start with pre-learned features instead of random weights |\n",
    "| **Less Training Data** | Leverage knowledge from the source domain |\n",
    "| **Better Performance** | Often achieves higher accuracy than training from scratch |\n",
    "| **Faster Convergence** | Models typically reach optimal performance in fewer epochs |\n",
    "| **Lower Computational Cost** | Requires fewer resources for training |\n",
    "| **Knowledge Retention** | Preserves useful features learned from large datasets |\n",
    "\n",
    "## 1.2 When to Use Transfer Learning\n",
    "***\n",
    "\n",
    "Transfer learning is particularly useful when:\n",
    "\n",
    "1. You have **limited training data** for your specific task\n",
    "2. The pre-trained model was trained on a **similar domain** to your target task\n",
    "3. You need to **accelerate model development**\n",
    "4. You have **limited computational resources**\n",
    "5. You want to **avoid overfitting** on small datasets\n",
    "\n",
    "> <img src=\"figs/icons/reminder.svg\" width=\"20\" style=\"filter: invert(100%) sepia(1500%) saturate(1500%) hue-rotate(30deg) brightness(450%) contrast(70%);\"/> **Key Insight**: The effectiveness of transfer learning depends on the similarity between the source and target domains. The more similar they are, the more beneficial transfer learning becomes.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quiz on Transfer Learning Concepts\n",
    "print(\"\\nðŸ§  Quiz 1: Transfer Learning Applications\")\n",
    "quizzer.run_quiz(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Case Study: Image Segmentation for Medical Imaging\n",
    "***\n",
    "> <img src=\"figs/icons/write.svg\" width=\"20\" style=\"filter: invert(41%) sepia(96%) saturate(1449%) hue-rotate(210deg) brightness(100%) contrast(92%);\"/> **Image Segmentation**: The process of partitioning an image into multiple segments or regions, often used in medical imaging to identify and delineate structures within images (e.g., tumors, organs). It is a crucial step in many computer vision tasks, including object detection and recognition.\n",
    "\n",
    "For this session, we are going to be using the [**ISIC 2016 Skin Lesion Segmentation Challenge**](https://challenge.isic-archive.com/landing/2016/) dataset. This dataset contains dermoscopic images of skin lesions, along with their corresponding segmentation masks. The goal is to train a model to accurately segment the lesions from the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(Path.cwd(), 'datasets')\n",
    "dataset_path = utils.data.download_dataset('skin lessions',\n",
    "                                           dest_path=data_path,\n",
    "                                           extract=True,\n",
    "                                           remove_compressed=False)\n",
    "mask_path = utils.data.download_dataset('skin lessions masks',\n",
    "                                   dest_path=data_path,\n",
    "                                   extract=True,\n",
    "                                   remove_compressed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Preparing the Dataset\n",
    "***\n",
    "Segmentation tasks require both the input images and their corresponding masks. The masks are binary images where the pixels belonging to the object of interest (e.g., a tumor) are marked as 1 (or white), while the background is marked as 0 (or black). Thus, we need to load both the images and their masks for training.\n",
    "\n",
    "In order for us to efficiently load the images and masks, we are going to create a custom dataset class. This class will inherit from the `torch.utils.data.Dataset` class and will handle loading the images and masks from the specified directories.\n",
    "\n",
    "***\n",
    "> <img src=\"figs/icons/code.svg\" width=\"20\" style=\"filter: invert(100%) sepia(100%) saturate(2000%) hue-rotate(40deg) brightness(915%) contrast(100%);\"/> **Snippet 1**: Create a custom dataset class for loading\n",
    "\n",
    "``` python\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes the dataset, loading the images and masks from the specified directories.\n",
    "        \"\"\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of images in the dataset.\n",
    "        \"\"\"\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Defines how to get a single item (image and mask) from the dataset.\n",
    "        \n",
    "```\n",
    "\n",
    "***\n",
    "The `Dataset` class requires us to implement three methods: \n",
    "| Method | Description |\n",
    "|--------|-------------|\n",
    "| `__init__` | Initializes the dataset, loading the images and masks from the specified directories. |\n",
    "| `__len__` | Returns the number of images in the dataset. |\n",
    "| `__getitem__` | Defines how to get a single item (image and mask) from the dataset. This can include loading the image, applying transformations, and returning the image and mask as tensors. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ISICDataset(Dataset):\n",
    "    def __init__(self, image_dir: Path | str, mask_dir: Path | str, img_transform=None, mask_transform=None):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.mask_dir = Path(mask_dir)\n",
    "        self.img_transform = img_transform\n",
    "        self.mask_transform = mask_transform\n",
    "        self.images = sorted(self.image_dir.glob(\"*.jpg\"))\n",
    "        self.masks = sorted(self.mask_dir.glob(\"*.png\"))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        img_path = self.image_dir / img_name\n",
    "        mask_name = str(img_name.stem + \"_segmentation.png\")\n",
    "        mask_path = self.mask_dir / mask_name\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\") # Convert to RGB\n",
    "        mask = Image.open(mask_path).convert(\"L\") # Convert to grayscale since it's a binary mask\n",
    "\n",
    "        # Apply transformations if provided\n",
    "        if self.img_transform:\n",
    "            image = self.img_transform(image)\n",
    "        if self.mask_transform:\n",
    "            mask = self.mask_transform(mask)\n",
    "        \n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Compute the Mean and Standard Deviation of the Dataset\n",
    "***\n",
    "\n",
    "We are going to compute the mean and standard deviation of the dataset. This is important because we will use these values to normalize the images before feeding them into the model. Normalization helps in speeding up the convergence of the model during training.\n",
    "\n",
    "First, we need to load the dataset and then compute the mean and standard deviation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "imgs_path = list(dataset_path.glob(\"*\"))[0]\n",
    "masks_path = list(mask_path.glob(\"*\"))[0]\n",
    "\n",
    "resize_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "ds = ISICDataset(\n",
    "    image_dir=imgs_path,\n",
    "    mask_dir=masks_path,\n",
    "    img_transform=resize_transform,\n",
    "    mask_transform=resize_transform\n",
    ")\n",
    "\n",
    "dl = DataLoader(ds, batch_size=16, shuffle=False)\n",
    "\n",
    "mean = 0.0\n",
    "std = 0.0\n",
    "n_samples = 0.0\n",
    "\n",
    "for data, _ in tqdm(dl, desc=\"Computing mean and std\", bar_format='{l_bar}{bar:20}{r_bar}{bar:-20b}'):\n",
    "    batch_samples = data.size(0)\n",
    "    data = data.view(batch_samples, data.size(1), -1)\n",
    "    mean += data.mean(2).sum(0)\n",
    "    std += data.std(2).sum(0)\n",
    "    n_samples += batch_samples\n",
    "\n",
    "mean /= n_samples\n",
    "std /= n_samples\n",
    "\n",
    "print(\"\\nMean:\", mean.numpy())\n",
    "print(\"Std:\", std.numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Data Augmentation\n",
    "***\n",
    "As we saw in the previous sessions, data augmentation is a technique used to artificially increase the size of the training dataset by applying random transformations to the images. This helps in improving the generalization of the model and reducing overfitting. \n",
    "\n",
    "We are going to use the albumentations library for data augmentation. This library outperforms the torchvision library in terms of speed and flexibility. It provides the same transformations as torchvision and it is also compatible with PyTorch.\n",
    "\n",
    "***\n",
    "> <img src=\"figs/icons/code.svg\" width=\"20\" style=\"filter: invert(100%) sepia(100%) saturate(2000%) hue-rotate(40deg) brightness(915%) contrast(100%);\"/> **Snippet 2**: Using albumentations for data augmentation\n",
    "\n",
    "``` python\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.Normalize(mean=mean, std=std, p=1.0),\n",
    "    ToTensorV2()\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "train_img_ts = A.Compose([\n",
    "    A.Resize(128, 128),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.Rotate(limit=10,p=0.5),\n",
    "    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),\n",
    "    A.Normalize(mean=mean.numpy(), std=std.numpy(), p=1.0),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "valid_img_ts = A.Compose([\n",
    "    A.Resize(128, 128),\n",
    "    A.Normalize(mean=mean.numpy(), std=std.numpy(), p=1.0),\n",
    "    ToTensorV2()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redifine the dataset with albumentations\n",
    "class ISICDatasetAlbumentations(ISICDataset):\n",
    "    def __init__(self, image_dir: Path | str, mask_dir: Path | str, transform=None):\n",
    "        super().__init__(image_dir, mask_dir, transform, None)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        img_path = self.image_dir / img_name\n",
    "        mask_name = str(img_name.stem + \"_segmentation.png\")\n",
    "        mask_path = self.mask_dir / mask_name\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\") # Convert to RGB\n",
    "        image = np.array(image)  # Convert to numpy array for albumentations\n",
    "        mask = Image.open(mask_path).convert(\"L\") # Convert to grayscale since it's a binary mask\n",
    "        mask = np.array(mask)\n",
    "\n",
    "        # Normalize mask to 0-1 range if it's not already\n",
    "        if mask.max() > 1:\n",
    "            mask = mask / 255.0\n",
    "\n",
    "        # Apply transformations if provided\n",
    "        if self.img_transform:\n",
    "            aug = self.img_transform(image=image, mask=mask)\n",
    "            image = aug['image']\n",
    "            mask = aug['mask']\n",
    "            \n",
    "            # Ensure mask is binary (0 or 1)\n",
    "            if isinstance(mask, torch.Tensor):\n",
    "                mask = (mask > 0.5).float()\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ds = ISICDatasetAlbumentations(\n",
    "    image_dir=imgs_path,\n",
    "    mask_dir=masks_path,\n",
    "    transform=train_img_ts\n",
    ")\n",
    "\n",
    "train_size = int(0.8 * len(full_ds))\n",
    "valid_size = len(full_ds) - train_size\n",
    "train_ds, valid_ds = torch.utils.data.random_split(full_ds, [train_size, valid_size])\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(dl, n=4):\n",
    "    \"\"\"Show a batch of images and masks.\"\"\"\n",
    "    for images, masks in dl:\n",
    "        fig, ax = plt.subplots(nrows=2, ncols=n, figsize=(20, 5))\n",
    "        for i in range(n):\n",
    "            # Denormalize the image\n",
    "            img = images[i].permute(1, 2, 0).numpy()\n",
    "            img = img * std.numpy() + mean.numpy()\n",
    "            img = np.clip(img, 0, 1)  # Clip values to valid range\n",
    "            \n",
    "            ax[0, i].imshow(img)\n",
    "            ax[0, i].set_title(f\"Image {i+1}\")\n",
    "            ax[0, i].axis('off')\n",
    "            ax[1, i].imshow(masks[i].squeeze(), cmap='gray')\n",
    "            ax[1, i].set_title(f\"Mask {i+1}\")\n",
    "            ax[1, i].axis('off')\n",
    "        plt.show()\n",
    "        break\n",
    "\n",
    "show_batch(train_dl, n=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Baseline Model U-Net\n",
    "***\n",
    "> <img src=\"figs/icons/write.svg\" width=\"20\" style=\"filter: invert(41%) sepia(96%) saturate(1449%) hue-rotate(210deg) brightness(100%) contrast(92%);\"/> **U-Net**: A convolutional neural network architecture designed for biomedical image segmentation. It consists of a contracting path (encoder) and an expansive path (decoder), allowing it to capture both context and localization information.\n",
    "\n",
    "The U-Net architecture is widely used in medical image segmentation tasks due to its ability to learn both local and global features. The architecture can is shown below:\n",
    "\n",
    "<figure style=\"background-color: white; border-radius: 10px; padding: 20px; text-align: center; margin: 0 auto;\">\n",
    "    <img src=\"figs/unet.png\" alt=\"U-Net Architecture\" style=\"width: 50%; height: auto; margin: 0 auto;\">\n",
    "</figure>\n",
    "\n",
    "The U-Net architecture consists of two main parts: the encoder and the decoder. The encoder captures context information, while the decoder enables precise localization. The skip connections between the encoder and decoder help retain spatial information lost during downsampling.\n",
    "\n",
    "For this architecture we are going to use a special type of convolutional layer that upsamples the input feature maps. This layer is called a transposed convolutional layer (also known as a deconvolutional layer). It is used to increase the spatial dimensions of the input feature maps, allowing the model to learn more complex features.\n",
    "\n",
    "We are going to implement the U-Net architecture using PyTorch. The implementation will consist of the following components:\n",
    "\n",
    "| Component | Description |\n",
    "|----------|-------------|\n",
    "| `DoubleConv` | A block that consists of two convolutional layers followed by batch normalization and ReLU activation. |\n",
    "| `Down` | A block that consists of a max pooling layer followed by a `DoubleConv` block. |\n",
    "| `Up` | A block that consists of a transposed convolutional layer followed by a `DoubleConv` block. |\n",
    "| `UNet` | The main U-Net architecture that consists of the encoder and decoder blocks. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unet model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "    \n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # Encoder: Downsampling path\n",
    "        self.encoder1 = DoubleConv(in_channels, 64)\n",
    "        self.encoder2 = DoubleConv(64, 128)\n",
    "        self.encoder3 = DoubleConv(128, 256)\n",
    "        self.encoder4 = DoubleConv(256, 512)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = DoubleConv(512, 1024)\n",
    "\n",
    "        # Decoder: Upsampling path\n",
    "        self.upconv4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.decoder4 = DoubleConv(1024, 512)\n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.decoder3 = DoubleConv(512, 256)\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder2 = DoubleConv(256, 128)\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.decoder1 = DoubleConv(128, 64)\n",
    "\n",
    "        # Output layer\n",
    "        self.output_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(F.max_pool2d(enc1, 2))\n",
    "        enc3 = self.encoder3(F.max_pool2d(enc2, 2))\n",
    "        enc4 = self.encoder4(F.max_pool2d(enc3, 2))\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(F.max_pool2d(enc4, 2))\n",
    "\n",
    "        # Decoder with skip connections\n",
    "        dec4 = self.upconv4(bottleneck)\n",
    "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
    "        dec4 = self.decoder4(dec4)\n",
    "\n",
    "        dec3 = self.upconv3(dec4)\n",
    "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "\n",
    "        dec2 = self.upconv2(dec3)\n",
    "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "\n",
    "        dec1 = self.upconv1(dec2)\n",
    "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "\n",
    "        # Output layer\n",
    "        output = self.output_conv(dec1)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Segmentation Loss Function\n",
    "***\n",
    "While we can use a simple loss function like binary cross-entropy for segmentation tasks since we are dealing with binary masks, it is often not sufficient. This is because the model may learn to predict the background class (0) more often than the foreground class (1), leading to poor performance on the actual segmentation task.\n",
    "\n",
    "To address this, we are going to introduce a more sophisticated loss function called the **Dice Loss**. The Dice Loss is based on the Dice coefficient, which measures the overlap between two sets. It is defined as:\n",
    "\n",
    "$$\n",
    "\\text{Dice} = \\frac{2 |X \\cap Y|}{|X| + |Y|}$$\n",
    "\n",
    "Where:\n",
    "- \\(X\\) is the predicted segmentation mask\n",
    "- \\(Y\\) is the ground truth segmentation mask\n",
    "- \\(|X|\\) is the number of pixels in the predicted mask\n",
    "- \\(|Y|\\) is the number of pixels in the ground truth mask\n",
    "- \\(|X \\cap Y|\\) is the number of pixels in the intersection of the predicted and ground truth masks\n",
    "\n",
    "The Dice Loss is defined as:\n",
    "$$\n",
    "\\text{Dice Loss} = 1 - \\text{Dice}\n",
    "$$\n",
    "\n",
    "This loss function is particularly useful for imbalanced datasets, where the number of pixels in the foreground class is much smaller than the number of pixels in the background class. The Dice Loss penalizes the model more for misclassifying foreground pixels than background pixels, leading to better performance on the segmentation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(torch.nn.Module):\n",
    "    def __init__(self, smooth=1.0):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Ensure inputs are properly shaped\n",
    "        y_pred = y_pred.view(-1)\n",
    "        y_true = y_true.view(-1).float()  # Ensure mask is float type\n",
    "        \n",
    "        # Calculate intersection and union\n",
    "        intersection = (y_pred * y_true).sum()\n",
    "        union = y_pred.sum() + y_true.sum()\n",
    "        \n",
    "        # Calculate Dice coefficient with smoothing factor\n",
    "        dice = (2. * intersection + self.smooth) / (union + self.smooth)\n",
    "        \n",
    "        # Return loss (1 - dice), clamped to ensure it's not negative\n",
    "        return torch.clamp(1 - dice, 0.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, num_epochs):\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", bar_format='{l_bar}{bar:20}{r_bar}{bar:-20b}'):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device).float()  # Ensure mask is float tensor\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f\"Training Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, masks in valid_loader:\n",
    "                images = images.to(device)\n",
    "                masks = masks.to(device).float()  # Ensure mask is float tensor\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "\n",
    "        epoch_val_loss = val_loss / len(valid_loader.dataset)\n",
    "        print(f\"Validation Loss: {epoch_val_loss:.4f}\")\n",
    "        \n",
    "        # Save model if validation loss improves\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            torch.save(model.state_dict(), 'best_unet_model.pt')\n",
    "            print(f\"Model saved. New best validation loss: {best_val_loss:.4f}\")\n",
    "            \n",
    "        print(\"-\" * 30)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Initialize the model, criterion, and optimizer\n",
    "model = UNet(in_channels=3, out_channels=1).to(device)\n",
    "criterion = DiceLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 5\n",
    "model = train_model(model, train_dl, valid_dl, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, dataloader, num_images=4):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, masks in dataloader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            # Apply threshold for binary segmentation\n",
    "            outputs = (outputs > 0.5).float()\n",
    "\n",
    "            fig, ax = plt.subplots(nrows=num_images, ncols=3, figsize=(15, 5 * num_images))\n",
    "            for i in range(num_images):\n",
    "                # Denormalize the image\n",
    "                img = images[i].cpu().permute(1, 2, 0).numpy()\n",
    "                img = img * std.numpy() + mean.numpy()\n",
    "                img = np.clip(img, 0, 1)  # Clip values to valid range\n",
    "\n",
    "                ax[i, 0].imshow(img)\n",
    "                ax[i, 0].set_title(f\"Image {i+1}\")\n",
    "                ax[i, 0].axis('off')\n",
    "\n",
    "                ax[i, 1].imshow(masks[i].squeeze().cpu().numpy(), cmap='gray')\n",
    "                ax[i, 1].set_title(f\"True Mask {i+1}\")\n",
    "                ax[i, 1].axis('off')\n",
    "\n",
    "                ax[i, 2].imshow(outputs[i].squeeze().cpu().numpy(), cmap='gray')\n",
    "                ax[i, 2].set_title(f\"Predicted Mask {i+1}\")\n",
    "                ax[i, 2].axis('off')\n",
    "\n",
    "            plt.show()\n",
    "            break\n",
    "\n",
    "# Add a cell to print some diagnostic information\n",
    "for images, masks in valid_dl:\n",
    "    print(f\"Image batch shape: {images.shape}, dtype: {images.dtype}, range: [{images.min().item():.4f}, {images.max().item():.4f}]\")\n",
    "    print(f\"Mask batch shape: {masks.shape}, dtype: {masks.dtype}, range: [{masks.min().item():.4f}, {masks.max().item():.4f}]\")\n",
    "    print(f\"Unique values in mask: {torch.unique(masks)}\")\n",
    "    break\n",
    "\n",
    "visualize_predictions(model, valid_dl, num_images=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Transfer Learning with Pre-trained Models\n",
    "***\n",
    "\n",
    "> <img src=\"figs/icons/write.svg\" width=\"20\" style=\"filter: invert(41%) sepia(96%) saturate(1449%) hue-rotate(210deg) brightness(100%) contrast(92%);\"/> **Transfer Learning**: The process of taking a pre-trained model and fine-tuning it on a new task. This is particularly useful when the new task has limited training data or when the model is computationally expensive to train from scratch.\n",
    "\n",
    "In this section, we are going to use a pre-trained model for the segmentation task. We are going to use an EfficientNet model that has been pre-trained on the ImageNet dataset. The EfficientNet model is a state-of-the-art convolutional neural network architecture that achieves high accuracy with fewer parameters compared to other architectures. \n",
    "\n",
    "The EfficientNet model is going to act as the encoder part of the U-Net architecture. We are going to replace the encoder part of the U-Net architecture with the EfficientNet model. The decoder part of the U-Net architecture will remain the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pretrained model and other necessary libraries\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# UNet model with EfficientNet backbone - Fixed version\n",
    "class UNetEfficient(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1, pretrained=True):\n",
    "        super(UNetEfficient, self).__init__()\n",
    "        \n",
    "        # Load pretrained EfficientNet (using B0 for efficiency)\n",
    "        if pretrained:\n",
    "            # Use efficientnet_b0 which is more lightweight than larger variants\n",
    "            self.efficient_net = models.efficientnet_b0(pretrained=True)\n",
    "        else:\n",
    "            self.efficient_net = models.efficientnet_b0(pretrained=False)\n",
    "            \n",
    "        # Extract feature layers from EfficientNet\n",
    "        self.enc1 = nn.Sequential(*list(self.efficient_net.features)[:2])  # Initial block\n",
    "        self.enc2 = self.efficient_net.features[2]                         # MBConv block 2\n",
    "        self.enc3 = self.efficient_net.features[3]                         # MBConv block 3\n",
    "        self.enc4 = self.efficient_net.features[4]                         # MBConv block 4\n",
    "        self.enc5 = nn.Sequential(                                         # MBConv blocks 5,6,7\n",
    "            self.efficient_net.features[5],\n",
    "            self.efficient_net.features[6],\n",
    "            self.efficient_net.features[7]\n",
    "        )\n",
    "        \n",
    "        # Get the output channels from each encoder stage\n",
    "        # These channels match the EfficientNet-B0 architecture\n",
    "        self.enc1_channels = 16     # Initial block outputs 16 channels\n",
    "        self.enc2_channels = 24     # MBConv block 2 outputs 24 channels\n",
    "        self.enc3_channels = 40     # MBConv block 3 outputs 40 channels\n",
    "        self.enc4_channels = 80     # MBConv block 4 outputs 80 channels\n",
    "        self.enc5_channels = 320    # Last MBConv block outputs 320 channels\n",
    "        \n",
    "        # Freeze encoder layers to preserve pretrained weights\n",
    "        encoders = [self.enc1, self.enc2, self.enc3, self.enc4, self.enc5]\n",
    "        for encoder in encoders:\n",
    "            for param in encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Center bottleneck (without changing spatial dimensions)\n",
    "        self.center = nn.Sequential(\n",
    "            nn.Conv2d(self.enc5_channels, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Upsampling path begins\n",
    "        self.up5 = nn.ConvTranspose2d(512, self.enc5_channels, kernel_size=2, stride=2)\n",
    "        \n",
    "        # Decoder paths\n",
    "        self.dec5 = nn.Sequential(\n",
    "            nn.Conv2d(self.enc5_channels*2, self.enc5_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(self.enc5_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(self.enc5_channels, self.enc5_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(self.enc5_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.up4 = nn.ConvTranspose2d(self.enc5_channels, self.enc4_channels, kernel_size=2, stride=2)\n",
    "        self.dec4 = nn.Sequential(\n",
    "            nn.Conv2d(self.enc4_channels*2, self.enc4_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(self.enc4_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(self.enc4_channels, self.enc4_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(self.enc4_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.up3 = nn.ConvTranspose2d(self.enc4_channels, self.enc3_channels, kernel_size=2, stride=2)\n",
    "        self.dec3 = nn.Sequential(\n",
    "            nn.Conv2d(self.enc3_channels*2, self.enc3_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(self.enc3_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(self.enc3_channels, self.enc3_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(self.enc3_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.up2 = nn.ConvTranspose2d(self.enc3_channels, self.enc2_channels, kernel_size=2, stride=2)\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.Conv2d(self.enc2_channels*2, self.enc2_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(self.enc2_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(self.enc2_channels, self.enc2_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(self.enc2_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.up1 = nn.ConvTranspose2d(self.enc2_channels, self.enc1_channels, kernel_size=2, stride=2)\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.Conv2d(self.enc1_channels*2, self.enc1_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(self.enc1_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(self.enc1_channels, self.enc1_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(self.enc1_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Final output layer\n",
    "        self.final = nn.Conv2d(self.enc1_channels, out_channels, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Save input size for later resizing if needed\n",
    "        input_size = x.size()[2:]\n",
    "        \n",
    "        # Encoder path with EfficientNet\n",
    "        e1 = self.enc1(x)         # Stage 1 features - 64x64x16\n",
    "        e2 = self.enc2(e1)        # Stage 2 features - 32x32x24\n",
    "        e3 = self.enc3(e2)        # Stage 3 features - 16x16x40\n",
    "        e4 = self.enc4(e3)        # Stage 4 features - 8x8x80\n",
    "        e5 = self.enc5(e4)        # Stage 5 features - 4x4x320\n",
    "        \n",
    "        # Center processing without changing spatial dimensions\n",
    "        c = self.center(e5)       # 4x4x512\n",
    "        \n",
    "        # Start upsampling and concatenating with skip connections\n",
    "        # Each step includes: upsampling, interpolation for skip connection if needed, concatenation, and convolutions\n",
    "        \n",
    "        # Level 5 - from 4x4 to 8x8\n",
    "        u5 = self.up5(c)          # 8x8x320\n",
    "        if u5.size()[2:] != e5.size()[2:]:\n",
    "            u5 = F.interpolate(u5, size=e5.size()[2:], mode='bilinear', align_corners=False)\n",
    "        d5 = self.dec5(torch.cat([u5, e5], dim=1))  # 8x8x320\n",
    "        \n",
    "        # Level 4 - from 8x8 to 16x16\n",
    "        u4 = self.up4(d5)         # 16x16x80\n",
    "        if u4.size()[2:] != e4.size()[2:]:\n",
    "            u4 = F.interpolate(u4, size=e4.size()[2:], mode='bilinear', align_corners=False)\n",
    "        d4 = self.dec4(torch.cat([u4, e4], dim=1))  # 16x16x80\n",
    "        \n",
    "        # Level 3 - from 16x16 to 32x32\n",
    "        u3 = self.up3(d4)         # 32x32x40\n",
    "        if u3.size()[2:] != e3.size()[2:]:\n",
    "            u3 = F.interpolate(u3, size=e3.size()[2:], mode='bilinear', align_corners=False)\n",
    "        d3 = self.dec3(torch.cat([u3, e3], dim=1))  # 32x32x40\n",
    "        \n",
    "        # Level 2 - from 32x32 to 64x64\n",
    "        u2 = self.up2(d3)         # 64x64x24\n",
    "        if u2.size()[2:] != e2.size()[2:]:\n",
    "            u2 = F.interpolate(u2, size=e2.size()[2:], mode='bilinear', align_corners=False)\n",
    "        d2 = self.dec2(torch.cat([u2, e2], dim=1))  # 64x64x24\n",
    "        \n",
    "        # Level 1 - from 64x64 to 128x128\n",
    "        u1 = self.up1(d2)         # 128x128x16\n",
    "        if u1.size()[2:] != e1.size()[2:]:\n",
    "            u1 = F.interpolate(u1, size=e1.size()[2:], mode='bilinear', align_corners=False)\n",
    "        d1 = self.dec1(torch.cat([u1, e1], dim=1))  # 128x128x16\n",
    "        \n",
    "        # Final output layer\n",
    "        output = self.final(d1)   # 128x128x1\n",
    "        \n",
    "        # Resize to match original input dimensions if needed\n",
    "        if output.size()[2:] != input_size:\n",
    "            output = F.interpolate(output, size=input_size, mode='bilinear', align_corners=False)\n",
    "            \n",
    "        return F.sigmoid(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the EfficientNet UNet model\n",
    "model_efficient = UNetEfficient(in_channels=3, out_channels=1).to(device)\n",
    "criterion_efficient = DiceLoss()\n",
    "optimizer_efficient = torch.optim.Adam(filter(lambda p: p.requires_grad, model_efficient.parameters()), lr=1e-4)\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "model_efficient = train_model(model_efficient, train_dl, valid_dl, criterion_efficient, optimizer_efficient, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions with EfficientNet UNet\n",
    "visualize_predictions(model_efficient, valid_dl, num_images=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_fse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
