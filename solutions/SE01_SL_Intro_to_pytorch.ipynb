{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch\n",
    "\n",
    "[PyTorch](https://pytorch.org/) is an open source machine learning and deep learning framework based on the Torch library.\n",
    "\n",
    "## Why PyTorch?\n",
    "\n",
    "PyTorch is a popular starting point for deep learning research due to its flexibility and ease of use, compared to other frameworks like TensorFlow. It is often recommended to use TensorFlow for **production-level projects**, but PyTorch is a great choice for **research and experimentation**. More explicitly, PyTorch has the following advantages:\n",
    "\n",
    "- **Dynamic computation graph**: PyTorch uses a dynamic computation graph, which means that the graph is generated on-the-fly as operations are created. This is in contrast to TensorFlow, which uses a static computation graph. The dynamic computation graph in PyTorch makes it easier to debug and understand the code.\n",
    "\n",
    "- **Pythonic**: PyTorch is designed to be Pythonic, which means that it is easy to read and write. This is in contrast to TensorFlow, which uses a more verbose syntax.\n",
    "\n",
    "- **Imperative programming**: PyTorch uses imperative programming, which means that you can write code that looks like regular Python code. This is in contrast to TensorFlow, which uses declarative programming.\n",
    "\n",
    "## Setting up the working environment\n",
    "\n",
    "We are going to use different python modules throughout this course. It is not necessary to be familiar with all of them at the moment. Some of these libraries enable us to work with data and perform numerical operations, while others are used for visualization purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faculty of Science and Engineering ðŸ”¬\n",
      "\u001b[95mThe University of Manchester \u001b[0m\n",
      "Invoking utils version: \u001b[92m0.7.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "helper_utils = Path.cwd().parent\n",
    "sys.path.append(str(helper_utils))\n",
    "\n",
    "from utils.data import download_dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to tensors\n",
    "\n",
    "> **Tensor**: A tensor is a generalisation of vectors and matrices that can have an arbitrary number of dimensions. In simple terms, a tensor is a multidimensional array.\n",
    "\n",
    "Similar to arrays, tensors can have different shapes and sizes. The number of dimensions of a tensor is called its **rank**. Here are some examples of tensors:\n",
    "\n",
    "- **Scalar**: A scalar is a single number, denoted as a tensor of rank 0.\n",
    "- **Vector**: A vector is an array of numbers, denoted as a tensor of rank 1.\n",
    "- **Matrix**: A matrix is a 2D array of numbers, denoted as a tensor of rank 2.\n",
    "- **3D tensor**: A 3D tensor is a cube of numbers, denoted as a tensor of rank 3.\n",
    "- **nD tensor**: An nD tensor is a generalisation of the above examples, denoted as a tensor of rank n.\n",
    "\n",
    "<figure style=\"background-color: white; border-radius: 10px; padding: 20px; text-align: center; margin: 0 auto;\">\n",
    "    <img src=\"..\\figs\\tensors.png\" alt=\"Visual representation of tensors\" align=\"center\" style=\"width: 60%; height: auto; margin: 0 auto;\" />\n",
    "</figure>\n",
    "\n",
    "\n",
    "The power of tensors comes in the form of their operations. Tensors can be added, multiplied, and manipulated in various ways. In the next section, we will see how to create tensors using PyTorch.\n",
    "\n",
    "## Creating tensors\n",
    "To create a tensor in PyTorch, we can use the class `torch.Tensor`\n",
    "\n",
    "> ðŸ“š **Documentation**: [torch.Tensor](https://pytorch.org/docs/stable/tensors.html)\n",
    "\n",
    "We are going to create a scalar tensor using a random integer value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalar value: 61\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(61), torch.Size([]), 0, torch.int64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a random integer\n",
    "rand_int = random.randint(0, 100)\n",
    "\n",
    "# Create a tensor from the random integer\n",
    "scalar = torch.tensor(rand_int)\n",
    "\n",
    "print(f'Scalar value: {scalar}')\n",
    "\n",
    "scalar, scalar.shape, scalar.ndim, scalar.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32.6290)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([]), 0, torch.float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Float scalar\n",
    "rand_float = random.uniform(0, 100)\n",
    "scalar = torch.tensor(rand_float)\n",
    "print(scalar)\n",
    "scalar.shape, scalar.ndim, scalar.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we create a tensor, we are creating a python object that represents the multidimensional array. As any other python object, the tensor has attributes and methods that we can use to manipulate it.\n",
    "\n",
    "In the above example, we created a scalar tensor with a single element. Looking at its attributes, we can see that the tensor has a shape of `torch.Size([])`, which means that it has no dimensions. We can also see that the tensor has a data type of `torch.int64`, which means that it is an integer tensor.\n",
    "\n",
    "> **Note**: The data type of a tensor is determined by the data type of the elements that it contains. It is important to be aware of the data type of a tensor, as it can affect the results of operations that are performed on it. Good practice is to always specify the data type of a tensor when creating it.\n",
    "\n",
    "As we can see our single element is now stored in a type of container, which means that we can perform operations on it but not directly on the element itself. To access the element, we can use the method `item()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.62903594970703"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can specify the data type of a tensor by passing the `dtype` argument to the `torch.Tensor` constructor. Alternatively, we can use the 'torch.tensor.type` method to change the data type of a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(42.)\n",
      "tensor(42)\n",
      "tensor(42, dtype=torch.int32)\n",
      "tensor(42., dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Create a scalar tensor with a specific data type\n",
    "scalar_tensor = torch.tensor(42, dtype=torch.float32)\n",
    "print(scalar_tensor)\n",
    "\n",
    "# Change the data type of a tensor\n",
    "scalar_tensor = scalar_tensor.type(torch.int64)\n",
    "print(scalar_tensor)\n",
    "\n",
    "# Another way to change the data type of a tensor\n",
    "scalar_tensor = scalar_tensor.int()\n",
    "print(scalar_tensor)\n",
    "\n",
    "# # Not recommended as it can be confusing \n",
    "# with the .to() method that is used to move tensors\n",
    "# to different devices\n",
    "scalar_tensor = scalar_tensor.to(torch.float64) \n",
    "print(scalar_tensor) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing tensors\n",
    "\n",
    "PyTorch has several functions to create tensors with different initial values. This is useful when we want to create tensors with specific shapes to represent data. For example, we can create a tensor of zeros, ones, or random values. Here are some examples of how to create tensors with different initial values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a 1D tensor using a range\n",
    "torch.arange(0, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a 3x3x3 tensor of zeros\n",
    "torch.zeros(3, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a 3x3x3 tensor of ones\n",
    "torch.ones(3, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5284, 0.5199, 0.2040],\n",
       "         [0.5786, 0.1376, 0.0844],\n",
       "         [0.3773, 0.9143, 0.8173]],\n",
       "\n",
       "        [[0.6608, 0.4748, 0.3692],\n",
       "         [0.0484, 0.1147, 0.9300],\n",
       "         [0.7445, 0.5994, 0.3852]],\n",
       "\n",
       "        [[0.6493, 0.7546, 0.5238],\n",
       "         [0.3814, 0.9689, 0.6665],\n",
       "         [0.7511, 0.0962, 0.9858]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a 3x3x3 tensor of random numbers using a uniform distribution between 0 and 1\n",
    "torch.rand(3, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a 3x3 diagonal tensor with 1s on the diagonal\n",
    "torch.eye(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  2.5000,  5.0000,  7.5000, 10.0000])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a 1D tensor with 5 evenly spaced values between 0 and 10\n",
    "torch.linspace(0, 10, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing tensors\n",
    "\n",
    "In order to access the elements of a tensor we can use the same indexing methods used for lists and numpy arrays. We can use the `[]` operator to access the elements of a tensor. \n",
    "\n",
    "> **Note:** The indexing is zero-based, which means that the first element has an index of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element at [0, 1, 2]: 6\n",
      "Element at [1, 0, 1]: 11\n"
     ]
    }
   ],
   "source": [
    "myTensor = torch.tensor(\n",
    "    [[[1, 2, 3],\n",
    "      [4, 5, 6],\n",
    "      [7, 8, 9]],\n",
    "     [[10, 11, 12],\n",
    "     [13, 14, 15],\n",
    "     [16, 17, 18]]])\n",
    "\n",
    "\n",
    "print(f'Element at [0, 1, 2]: {myTensor[0, 1, 2]}')\n",
    "print(f'Element at [1, 0, 1]: {myTensor[1, 0, 1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the `:` operator to access a range of elements. This is similar to how we access elements in lists and numpy arrays. The `:` operator allows us to specify a range of indices to access a subset of the tensor.\n",
    "\n",
    "Furthermore, we can use the `...` operator to access all elements in a tensor. This is useful when we want to access all elements in a specific dimension of a tensor. \n",
    "\n",
    "Finally, we can use negative indexing to access elements from the end of a tensor. For instance, we can use `-2` to access the second-to-last element of a tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[14, 15],\n",
      "        [17, 18]])\n"
     ]
    }
   ],
   "source": [
    "# Slicing\n",
    "print(myTensor[1, 1:3, 1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5],\n",
      "        [8]])\n"
     ]
    }
   ],
   "source": [
    "# Slicing with step\n",
    "print(myTensor[0, 1:3, 1:3:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1,  2],\n",
      "         [ 4,  5],\n",
      "         [ 7,  8]],\n",
      "\n",
      "        [[10, 11],\n",
      "         [13, 14],\n",
      "         [16, 17]]])\n"
     ]
    }
   ],
   "source": [
    "# Slicing with ellipsis\n",
    "# Ellipsis (...) can be used to represent multiple colons\n",
    "print(myTensor[..., 0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3,  6,  9],\n",
      "        [12, 15, 18]])\n"
     ]
    }
   ],
   "source": [
    "# Negative indexing\n",
    "print(myTensor[..., -1])  # Last element in the last dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor operations\n",
    "\n",
    "PyTorch allows us to manipulate tensors in different ways. Since PyTorch is built on top of NumPy, the same operations can be accessed through the `torch` module or alternatively through the `numpy` module. Due to the pythonic nature of PyTorch, we can also use the same operations as we would in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1, 2], [3, 4]])\n",
    "b = torch.tensor([[5, 6], [7, 8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition:\n",
      "--------------------\n",
      "tensor([[ 6,  8],\n",
      "        [10, 12]]) \n",
      "\n",
      "Subtraction:\n",
      "--------------------\n",
      "tensor([[-4, -4],\n",
      "        [-4, -4]]) \n",
      "\n",
      "Multiplication:\n",
      "--------------------\n",
      "tensor([[ 5, 12],\n",
      "        [21, 32]]) \n",
      "\n",
      "Division:\n",
      "--------------------\n",
      "tensor([[0.2000, 0.3333],\n",
      "        [0.4286, 0.5000]]) \n",
      "\n",
      "Exponentiation:\n",
      "--------------------\n",
      "tensor([[    1,    64],\n",
      "        [ 2187, 65536]]) \n",
      "\n",
      "Square root:\n",
      "--------------------\n",
      "tensor([[1.0000, 1.4142],\n",
      "        [1.7321, 2.0000]]) \n",
      "\n",
      "Logarithm:\n",
      "--------------------\n",
      "tensor([[0.0000, 0.6931],\n",
      "        [1.0986, 1.3863]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tensor Addition\n",
    "c = a + b\n",
    "print(f'Addition:\\n' + '-' * 20)\n",
    "print(c, '\\n')\n",
    "\n",
    "# Tensor subtraction\n",
    "c = a - b\n",
    "print(f'Subtraction:\\n' + '-' * 20)\n",
    "print(c, '\\n')\n",
    "\n",
    "# Tensor multiplication\n",
    "c = a * b\n",
    "print(f'Multiplication:\\n' + '-' * 20)\n",
    "print(c, '\\n')\n",
    "\n",
    "# Tensor division\n",
    "c = a / b\n",
    "print(f'Division:\\n' + '-' * 20)\n",
    "print(c, '\\n')\n",
    "\n",
    "# Tensor exponentiation\n",
    "c = a ** b\n",
    "print(f'Exponentiation:\\n' + '-' * 20)\n",
    "print(c, '\\n')\n",
    "\n",
    "# Tensor square root\n",
    "c = a ** (1/2)\n",
    "print(f'Square root:\\n' + '-' * 20)\n",
    "print(c, '\\n')\n",
    "\n",
    "# Tensor logarithm\n",
    "c = torch.log(a)\n",
    "print(f'Logarithm:\\n' + '-' * 20)\n",
    "print(c, '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix operations\n",
    "\n",
    "Matrix multiplication is a common operation in algebra and is used in many machine learning algorithms. We can perform:\n",
    "\n",
    "- **Matrix multiplication**: This is the standard matrix multiplication operation, which is denoted by the `@` operator in Python. This operation is also known as the dot product.\n",
    "- **Element-wise multiplication**: This is the multiplication of two matrices of the same shape, which is denoted by the `*` operator in Python. This operation is also known as the Hadamard product.\n",
    "- **Matrix transpose**: This is the operation of flipping a matrix over its diagonal, which is denoted by the `.T` attribute in Python. This operation is also known as the matrix transpose.\n",
    "- **Matrix inverse**: This is the operation of finding the inverse of a matrix, which is denoted by the `torch.inverse()` function in Python. This operation is also known as the matrix inverse.\n",
    "\n",
    "<figure style=\"background-color: white; border-radius: 10px; padding: 20px; text-align: center; margin: 0 auto; display: flex; justify-content: center; align-items: center; overflow: hidden;\">\n",
    "    <img src=\"..\\figs\\matrix_mul.gif\" alt=\"Matrix Multiplication\" style=\"width: 40%; height: 220px; object-fit: cover;\">\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      "--------------------\n",
      "tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.]]) \n",
      "\n",
      "Matrix B:\n",
      "--------------------\n",
      "tensor([[5., 9., 4.],\n",
      "        [5., 0., 8.],\n",
      "        [0., 7., 0.]]) \n",
      "\n",
      "Matrix multiplication:\n",
      "--------------------\n",
      "tensor([[30., 48., 36.],\n",
      "        [30., 48., 36.],\n",
      "        [30., 48., 36.]]) \n",
      "\n",
      "Element-wise multiplication:\n",
      "--------------------\n",
      "tensor([[15., 27., 12.],\n",
      "        [15.,  0., 24.],\n",
      "        [ 0., 21.,  0.]]) \n",
      "\n",
      "Matrix transpose:\n",
      "--------------------\n",
      "tensor([[5., 5., 0.],\n",
      "        [9., 0., 7.],\n",
      "        [4., 8., 0.]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tensor matrix multiplication\n",
    "A = torch.ones(3, 3) * 3\n",
    "B = torch.randint(0, 10, (3, 3)).to(torch.float32)\n",
    "print(f'Matrix A:\\n' + '-' * 20)\n",
    "print(A, '\\n')\n",
    "print(f'Matrix B:\\n' + '-' * 20)\n",
    "print(B, '\\n')\n",
    "\n",
    "print(f'Matrix multiplication:\\n' + '-' * 20)\n",
    "print(A @ B, '\\n')  # Equivalent to A.matmul(B)\n",
    "\n",
    "print(f'Element-wise multiplication:\\n' + '-' * 20)\n",
    "print(A * B, '\\n') # Element-wise multiplication\n",
    "\n",
    "print(f'Matrix transpose:\\n' + '-' * 20)\n",
    "print(B.T, '\\n')  # Transpose of A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Broadcasting\n",
    "\n",
    "Since PyTorch is built on top of NumPy we can use its broadcasting capabilities. Broadcasting is how NumPy handles arrays with different shapes during arithmetic operations. It allows us to perform operations on arrays of different shapes without having to explicitly reshape them. This is done by automatically expanding the smaller array to match the shape of the larger array.\n",
    "\n",
    "For example, if we have a 1D array of shape `(3,)` and a 2D array of shape `(3, 2)`, we can add them together without having to reshape the 1D array. NumPy will automatically expand the 1D array to match the shape of the 2D array.\n",
    "\n",
    "> ðŸ“š **Documentation**: [Broadcasting](https://numpy.org/doc/stable/user/basics.broadcasting.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Broadcasting:\n",
      "--------------------\n",
      "tensor([[ 6,  7],\n",
      "        [ 9, 10]]) \n",
      "\n",
      "Broadcasting with multiplication:\n",
      "--------------------\n",
      "tensor([[ 5, 10],\n",
      "        [18, 24]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2], [3, 4]])\n",
    "b = torch.tensor([[5], [6]])\n",
    "\n",
    "# Broadcasting\n",
    "c = a + b\n",
    "print(f'Broadcasting:\\n' + '-' * 20)\n",
    "print(c, '\\n')\n",
    "\n",
    "c = a * b\n",
    "print(f'Broadcasting with multiplication:\\n' + '-' * 20)\n",
    "print(c, '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping tensors\n",
    "### Unsqueeze\n",
    "If a tensor is not broadcastable we can use different methods to make it broadcastable. For example, we can use the `unsqueeze` method to add a dimension to the tensor. The `unsqueeze` method takes an integer argument that specifies the dimension to add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: The size of tensor a (32) must match the size of tensor b (3) at non-singleton dimension 3\n",
      "Scaled images shape: torch.Size([8, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "images = torch.rand(8, 3, 32, 32)  #  batch of 8 images, 3 channels (RGB), 32x32 resolution\n",
    "scaling_factors = torch.tensor([0.5, 1.5, 2.0])  # Shape: (3,)\n",
    "\n",
    "try:\n",
    "    scaled_images = images * scaling_factors\n",
    "except RuntimeError as e:\n",
    "    print(f'Error: {e}')\n",
    "\n",
    "# To apply the scaling factors to each channel of the images, we need to unsqueeze the scaling_factors tensor\n",
    "scaling_factors = scaling_factors.unsqueeze(0).unsqueeze(2).unsqueeze(3)  # Shape: (1, 3, 1, 1)\n",
    "scaled_images = images * scaling_factors\n",
    "\n",
    "print(f'Scaled images shape: {scaled_images.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why unsqueeze(0), unsqueeze(2), and unsqueeze(3)?**\n",
    "\n",
    "When applying channel-specific scaling to images, we need to align the dimensions correctly:\n",
    "\n",
    "**1. Understanding the Shapes**\n",
    "- **Images**: `torch.Size([8, 3, 32, 32])`\n",
    "    - 8 images in the batch\n",
    "    - 3 channels (RGB)\n",
    "    - 32Ã—32 pixels per image\n",
    "\n",
    "- **Scaling Factors**: `torch.Size([3])`\n",
    "    - One scaling factor per channel\n",
    "\n",
    "**2. Dimension Transformation**\n",
    "- **Original**: `[3]` (just channel values)\n",
    "- **After unsqueeze(0)**: `[1, 3]` (adds batch dimension)\n",
    "- **After unsqueeze(2)**: `[1, 3, 1]` (adds height dimension)\n",
    "- **After unsqueeze(3)**: `[1, 3, 1, 1]` (adds width dimension)\n",
    "\n",
    "**3. Broadcasting in Action**\n",
    "- The `[1, 3, 1, 1]` tensor broadcasts to `[8, 3, 32, 32]`\n",
    "- Each scaling factor is applied to its corresponding channel across all images\n",
    "- Batch dimension (1â†’8), height (1â†’32), and width (1â†’32) dimensions are all broadcast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View\n",
    "The `view` method allows us to reshape a tensor while keeping the underlying memory layout. This is useful when we want to change the shape of a tensor without copying the data. The `view` method takes an integer argument that specifies the shape of the tensor after reshaping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled images shape: torch.Size([8, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "images = torch.rand(8, 3, 32, 32)  #  batch of 8 images, 3 channels (RGB), 32x32 resolution\n",
    "scaling_factors = torch.tensor([0.5, 1.5, 2.0])  # Shape: (3,)\n",
    "\n",
    "scaling_factors = scaling_factors.view(1, 3, 1, 1)\n",
    "scaled_images = images * scaling_factors\n",
    "print(f'Scaled images shape: {scaled_images.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expand\n",
    "The `expand` method allows us to expand the dimensions of a tensor without copying the data. This is useful when we want to create a tensor with a specific shape without having to copy the data. The `expand` method takes an integer argument that specifies the shape of the tensor after expansion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 1\n",
      "Expanded b shape: torch.Size([3, 4])\n",
      "Final result shape: torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(3,4) # Shape (3, 4)\n",
    "b = torch.rand(3) # Shape (3,)\n",
    "\n",
    "try:\n",
    "    c = a + b\n",
    "except RuntimeError as e:\n",
    "    print(f'Error: {e}')\n",
    "\n",
    "b_expanded = b.unsqueeze(1).expand(3, 4) # Shape (3, 1) -> (3, 4)\n",
    "c = a + b_expanded\n",
    "print(f'Expanded b shape: {b_expanded.shape}')\n",
    "print(f'Final result shape: {c.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape\n",
    "The `reshape` method works similarly to the `view` method, but it can also handle cases where the tensor is not contiguous in memory. The latter means that it may create a copy of the data if the memory layout is not compatible. The `reshape` method takes an integer argument that specifies the shape of the tensor after reshaping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix shape: torch.Size([5, 3])\n",
      "Reshaped vector shape: torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "matrix = torch.rand(5, 3)  # Shape: (5, 3)\n",
    "vector = torch.rand(5)     # Shape: (5,)\n",
    "\n",
    "# Reshape vector from (5,) to (5,1) to allow broadcasting over (5,3)\n",
    "vector_reshaped = vector.reshape(5, 1)\n",
    "result = matrix + vector_reshaped\n",
    "print(f'Matrix shape: {matrix.shape}')\n",
    "print(f'Reshaped vector shape: {vector_reshaped.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Method | Function |\n",
    "|--------|----------|\n",
    "| `unsqueeze(dim)` | Adds a singleton dimension at dim |\n",
    "| `expand(*sizes)` | Expands singleton dimensions without copying data |\n",
    "| `view(*sizes)` | Reshapes without copying memory (if possible) |\n",
    "| `reshape(*sizes)` | Reshapes, may create a new memory copy |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data to tensors\n",
    "\n",
    "As mentioned before, PyTorch inherent pythonic nature allows us to easily convert existing data structures to tensors. Thus, we can use different data science libraries to load data and convert it to tensors. We are going to use the `pandas` library to load data from CSV files and convert it to tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading:\n",
      "ARKOMA: The Dataset to Build Neural Networks-Based Inverse Kinematics for NAO Robot Arms\n",
      "> Authors: Arif Nugroho, Eko Mulyanto Yuniarno, Mauridhi Hery Purnomo\n",
      "> Year: 2020\n",
      "> Website: https://www.sciencedirect.com/science/article/pii/S2352340923007989\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading brg4dz8nbb-1.zip: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 658k/658k [00:00<00:00, 4.38MiB/s]\n",
      "Extracting brg4dz8nbb-1.zip: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 248.43it/s]\n",
      "Extracting Dataset on NAO Robot Arms.zip: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 806.02it/s]\n"
     ]
    }
   ],
   "source": [
    "data_path = Path(Path.cwd(), 'datasets')\n",
    "dataset_path = download_dataset('ARKOMA',\n",
    "                                   dest_path=data_path,\n",
    "                                   extract=True)\n",
    "\n",
    "dataset_path = dataset_path / 'Dataset on NAO Robot Arms' / 'Left Arm Dataset' / 'LTrain_x.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrames in pandas are similar to tables in SQL or Excel. They are two-dimensional data structures that can hold different types of data. DataFrames have rows and columns, where each column can have a different data type. We can use the `pandas` library to load data from CSV files and convert it to DataFrames.\n",
    "> ðŸ“š **Documentation**: [pandas](https://pandas.pydata.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "count",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "std",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "min",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "25%",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "50%",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "75%",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "max",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "199a3e35-d53c-4ccb-8135-5d465a9894ae",
       "rows": [
        [
         "Px",
         "6000.0",
         "98.92858333333334",
         "76.2261242665302",
         "-149.87",
         "45.2975",
         "116.345",
         "161.75",
         "218.63"
        ],
        [
         "Py",
         "6000.0",
         "179.5848933333333",
         "52.808400454994455",
         "-30.44",
         "145.8625",
         "180.81",
         "213.7625",
         "315.66"
        ],
        [
         "Pz",
         "6000.0",
         "88.93907166666668",
         "124.25250149920389",
         "-117.3",
         "-24.62",
         "74.065",
         "206.29",
         "318.99"
        ],
        [
         "Rx",
         "6000.0",
         "-4.468263333333334",
         "46.874867493335294",
         "-179.42",
         "-32.7",
         "-0.91",
         "26.762500000000003",
         "159.17"
        ],
        [
         "Ry",
         "6000.0",
         "-1.7391483333333335",
         "55.52564166285407",
         "-171.43",
         "-42.1325",
         "-0.305",
         "38.6725",
         "166.35"
        ],
        [
         "Rz",
         "6000.0",
         "4.639301666666666",
         "32.80761586388303",
         "-157.1",
         "-13.2125",
         "4.635",
         "23.5825",
         "153.66"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 6
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Px</th>\n",
       "      <td>6000.0</td>\n",
       "      <td>98.928583</td>\n",
       "      <td>76.226124</td>\n",
       "      <td>-149.87</td>\n",
       "      <td>45.2975</td>\n",
       "      <td>116.345</td>\n",
       "      <td>161.7500</td>\n",
       "      <td>218.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Py</th>\n",
       "      <td>6000.0</td>\n",
       "      <td>179.584893</td>\n",
       "      <td>52.808400</td>\n",
       "      <td>-30.44</td>\n",
       "      <td>145.8625</td>\n",
       "      <td>180.810</td>\n",
       "      <td>213.7625</td>\n",
       "      <td>315.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pz</th>\n",
       "      <td>6000.0</td>\n",
       "      <td>88.939072</td>\n",
       "      <td>124.252501</td>\n",
       "      <td>-117.30</td>\n",
       "      <td>-24.6200</td>\n",
       "      <td>74.065</td>\n",
       "      <td>206.2900</td>\n",
       "      <td>318.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rx</th>\n",
       "      <td>6000.0</td>\n",
       "      <td>-4.468263</td>\n",
       "      <td>46.874867</td>\n",
       "      <td>-179.42</td>\n",
       "      <td>-32.7000</td>\n",
       "      <td>-0.910</td>\n",
       "      <td>26.7625</td>\n",
       "      <td>159.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ry</th>\n",
       "      <td>6000.0</td>\n",
       "      <td>-1.739148</td>\n",
       "      <td>55.525642</td>\n",
       "      <td>-171.43</td>\n",
       "      <td>-42.1325</td>\n",
       "      <td>-0.305</td>\n",
       "      <td>38.6725</td>\n",
       "      <td>166.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rz</th>\n",
       "      <td>6000.0</td>\n",
       "      <td>4.639302</td>\n",
       "      <td>32.807616</td>\n",
       "      <td>-157.10</td>\n",
       "      <td>-13.2125</td>\n",
       "      <td>4.635</td>\n",
       "      <td>23.5825</td>\n",
       "      <td>153.66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     count        mean         std     min       25%      50%       75%  \\\n",
       "Px  6000.0   98.928583   76.226124 -149.87   45.2975  116.345  161.7500   \n",
       "Py  6000.0  179.584893   52.808400  -30.44  145.8625  180.810  213.7625   \n",
       "Pz  6000.0   88.939072  124.252501 -117.30  -24.6200   74.065  206.2900   \n",
       "Rx  6000.0   -4.468263   46.874867 -179.42  -32.7000   -0.910   26.7625   \n",
       "Ry  6000.0   -1.739148   55.525642 -171.43  -42.1325   -0.305   38.6725   \n",
       "Rz  6000.0    4.639302   32.807616 -157.10  -13.2125    4.635   23.5825   \n",
       "\n",
       "       max  \n",
       "Px  218.63  \n",
       "Py  315.66  \n",
       "Pz  318.99  \n",
       "Rx  159.17  \n",
       "Ry  166.35  \n",
       "Rz  153.66  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the dataset\n",
    "df = pd.read_csv(dataset_path)\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the data as a numpy array\n",
    "type(df.Px.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-27.9200, 110.5800, 180.9600,  ..., 104.3300, 122.9200, 139.4300],\n",
       "        dtype=torch.float64),\n",
       " torch.Size([6000]),\n",
       " 1,\n",
       " torch.float64,\n",
       " device(type='cpu'))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the numpy array to a PyTorch tensor\n",
    "px_tensor = torch.tensor(df.Px.values)\n",
    "px_tensor, px_tensor.shape, px_tensor.ndim, px_tensor.dtype, px_tensor.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-27.9200, 110.5800, 180.9600,  ..., 104.3300, 122.9200, 139.4300],\n",
       "        dtype=torch.float64),\n",
       " torch.Size([6000]),\n",
       " 1,\n",
       " torch.float64,\n",
       " device(type='cpu'))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alternatively, we can use the from_numpy method\n",
    "px_tensor = torch.from_numpy(df.Px.values)\n",
    "px_tensor, px_tensor.shape, px_tensor.ndim, px_tensor.dtype, px_tensor.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6000, 6]), 2, torch.float64, device(type='cpu'))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor from the entire DataFrame\n",
    "data = torch.tensor(df.values)\n",
    "data.shape, data.ndim, data.dtype, data.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the GPU\n",
    "PyTorch allows us to use the GPU to accelerate computations. This is done by moving the tensors to the GPU memory. We can do this by using the `to` method of a tensor and passing the device as an argument. The device can be either `cuda` or `cpu`. The `cuda` device refers to the GPU, while the `cpu` device refers to the CPU.\n",
    "\n",
    "> **Note**: Not all operations are supported on the GPU. If an operation is not supported on the GPU, PyTorch will automatically move the tensor to the CPU and perform the operation there. This can lead to performance issues, so it is important to be aware of which operations are supported on the GPU.\n",
    "\n",
    "## Checking for GPU availability\n",
    "We can check if a GPU is available by using the `torch.cuda.is_available()` method. This method returns a boolean value that indicates whether a GPU is available or not. If a GPU is available, we can use it to accelerate computations.\n",
    "\n",
    "## When to use the GPU\n",
    "Using the GPU is beneficial when we are working with large tensors or when we are performing operations that are computationally expensive. For example, training a deep learning model on a large dataset can be accelerated by using the GPU. However, if we are working with small tensors or performing simple operations, using the CPU may be faster. \n",
    "\n",
    "Typically, we use the GPU for computer vision and natural language processing tasks, where the data is large and the operations are computationally expensive.\n",
    "\n",
    "> **Note**: When choosing between the CPU and GPU, it is important to make sure that all tensors and models are on the same device. If a tensor is on the CPU and a model is on the GPU, PyTorch will automatically move the tensor to the GPU, which can lead to performance issues. It is important to be aware of which device each tensor and model is on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available: cuda\n",
      "Tensor moved to device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f'GPU is available: {device}')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(f'GPU is not available, using CPU: {device}')\n",
    "\n",
    "# Move the tensor to the GPU\n",
    "px_tensor = px_tensor.to(device)\n",
    "print(f'Tensor moved to device: {px_tensor.device}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_fse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
