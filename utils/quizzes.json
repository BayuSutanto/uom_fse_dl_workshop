{
    "SE03": {
      "quiz_1": {
        "title": "Activation Functions Quiz",
        "question": "For a regression problem like predicting the robot arm joint angles, which activation function would be most appropriate for the output layer?",
        "options": [
          "Sigmoid - to constrain all outputs between 0 and 1",
          "ReLU - to ensure no negative values in the output",
          "Linear (no activation) - to allow any numeric output value",
          "Softmax - to convert outputs into probability distributions",
          "Tanh - to constrain all outputs between -1 and 1"
        ],
        "correct": 2,
        "explanation": "For regression problems, where we need to predict continuous values like joint angles, the linear activation (or no activation) is most appropriate for the output layer. This is because:\n\n1. Regression outputs need the full range of possible values, not constrained to intervals like [0,1] (sigmoid) or [-1,1] (tanh).\n2. Joint angles can be positive or negative, so ReLU (which outputs only positive values) would be too restrictive.\n3. Softmax is for multi-class classification problems, not regression.\n\nHidden layers in the network can and should use non-linear activations like ReLU, Tanh, or Leaky ReLU to capture complex relationships, but the output layer in regression tasks typically uses linear activation to predict any real-valued output."
      },
      "quiz_2": {
        "title": "Neural Network Architecture Quiz",
        "question": "When designing a neural network for the robot arm inverse kinematics problem, which combination of activation functions would likely work best?",
        "options": [
          "Hidden layers: Sigmoid, Output layer: Sigmoid",
          "Hidden layers: ReLU, Output layer: Linear",
          "Hidden layers: Linear, Output layer: ReLU",
          "Hidden layers: Softmax, Output layer: Tanh",
          "Hidden layers: Tanh, Output layer: Softmax"
        ],
        "correct": 1,
        "explanation": "The best combination for this regression task is:\n\nHidden layers: ReLU - ReLU (Rectified Linear Unit) works well in hidden layers because:\n1. It helps mitigate the vanishing gradient problem that can occur with sigmoid/tanh\n2. It introduces non-linearity needed to learn complex patterns\n3. It's computationally efficient\n4. It's widely used in modern neural networks with proven success\n\nOutput layer: Linear - Linear activation is appropriate for the output layer in regression tasks because:\n1. It allows the model to predict any numerical value within the range of joint angles\n2. Joint angles can be positive or negative values\n3. The model needs to predict exact values, not probabilities or classifications\n\nSigmoid and tanh would constrain outputs inappropriately, while softmax is designed for multi-class classification problems."
      },
      "quiz_3": {
        "title": "Neural Network Width Quiz",
        "question": "For the robot arm inverse kinematics regression problem, which statement about network width (number of neurons in hidden layers) is most accurate?",
        "options": [
          "A single neuron in each hidden layer is sufficient since this is a simple regression task",
          "The number of neurons should exactly match the number of input features (6)",
          "The hidden layers should have more neurons than inputs to capture complex spatial relationships",
          "The hidden layers should have fewer neurons than the output to prevent overfitting",
          "The width of the network doesn't impact performance, only the depth matters"
        ],
        "correct": 2,
        "explanation": "The hidden layers should have more neurons than inputs to capture complex spatial relationships in the robot arm kinematics problem. This is because:\n\n1. Inverse kinematics involves complex non-linear relationships between end-effector positions and joint angles\n2. The mapping from 6D input space (position and orientation) to 5D output space (joint angles) requires learning complex mathematical transformations\n3. With too few neurons, the network would suffer from high bias (underfitting)\n4. The number of neurons doesn't need to match the input or output dimensions exactly\n5. While having too many neurons can lead to overfitting, techniques like regularization and dropout can help mitigate this\n\nA common practice is to start with more neurons than inputs and gradually adjust based on validation performance."
      },
      "quiz_4": {
        "title": "Network Depth for Inverse Kinematics",
        "question": "When designing a neural network for robot arm inverse kinematics, which statement about network depth (number of hidden layers) is most accurate?",
        "options": [
          "A single hidden layer is always sufficient for any regression problem",
          "Deeper networks are always better than shallow ones for inverse kinematics",
          "Multiple hidden layers help capture hierarchical relationships in joint movements",
          "Deeper networks train faster than shallow networks",
          "The number of hidden layers should match the number of robot joints"
        ],
        "correct": 2,
        "explanation": "Multiple hidden layers help capture hierarchical relationships in joint movements. This is because:\n\n1. Inverse kinematics involves complex geometric and spatial transformations that benefit from hierarchical representations\n2. The first few layers might learn basic spatial features while deeper layers combine these into more complex movement patterns\n3. With a single hidden layer, the model might struggle to approximate the complex non-linear relationship between end-effector positions and joint configurations\n4. However, very deep networks may be prone to training difficulties like vanishing gradients\n5. The optimal depth depends on the complexity of the specific robot's kinematic chain, not just the number of joints\n\nDepth should be chosen based on the complexity of the task and validated empirically, not based on arbitrary rules."
      },
      "quiz_5": {
        "title": "Regularization Techniques for Kinematics Models",
        "question": "Which regularization technique would be most effective for improving generalization in a neural network for robot arm inverse kinematics?",
        "options": [
          "Using only linear layers without any non-linear activations",
          "Implementing dropout between layers during training",
          "Limiting training to exactly 10 epochs",
          "Using MSE loss instead of MAE loss",
          "Removing all hidden layers to simplify the model"
        ],
        "correct": 1,
        "explanation": "Implementing dropout between layers during training is most effective because:\n\n1. Dropout randomly deactivates a percentage of neurons during each training step, which prevents neurons from co-adapting too much\n2. This forces the network to learn redundant representations of the kinematic relationships\n3. The model becomes more robust to variations in input positions and orientations\n4. Dropout acts like an ensemble of different network architectures, improving generalization\n5. It helps prevent overfitting when the network needs to be expressive (with many parameters) to capture the complex mapping\n\nFor robot arm inverse kinematics, good generalization is critical since the robot needs to perform reliably across the entire workspace, even in positions not exactly represented in the training data."
      }
    },
    "SE04": {
        "quiz_1": {
            "title": "CNN Components Quiz",
            "question": "Which of the following statements about convolutional layers is TRUE?",
            "options": [
                "Convolutional layers always reduce the spatial dimensions of the input",
                "Each filter in a convolutional layer must have the same number of channels as the input",
                "The number of parameters in a convolutional layer depends on the size of the input image",
                "A 3x3 filter with stride 1 can only be applied to images with dimensions that are multiples of 3"
            ],
            "correct": 1,
            "explanation": "Each filter in a convolutional layer must have the same number of channels as the input. This is because the filter is applied across the entire depth of the input volume. The number of parameters in a convolutional layer depends on the filter size and number of filters, not on the input image size. Whether convolutional layers reduce dimensions depends on padding and stride settings. And filters can be applied to images of any dimension; they don't need to be multiples of the filter size."
        },
        "quiz_2": {
            "title": "CNN Architecture Quiz",
            "question": "Which CNN architecture introduced 'residual connections' to solve the vanishing gradient problem in very deep networks?",
            "options": [
                "AlexNet",
                "VGG-16",
                "ResNet",
                "Inception (GoogLeNet)"
            ],
            "correct": 2,
            "explanation": "ResNet (Residual Network) introduced residual connections (or skip connections) that allow gradients to flow directly through the network, helping to train very deep networks without suffering from vanishing gradients. This innovation enabled the successful training of networks with over 100 layers. AlexNet was the first CNN to win ImageNet, VGG-16 used small filters throughout the network, and Inception used modules with multiple filter sizes in parallel."
        },
        "quiz_3": {
            "title": "CNN Training Best Practices",
            "question": "When training a CNN from scratch with limited data, which approach is MOST likely to improve generalization?",
            "options": [
                "Using a very large model with many parameters",
                "Applying aggressive data augmentation techniques",
                "Removing all regularization methods like dropout",
                "Training with a very high learning rate"
            ],
            "correct": 1,
            "explanation": "With limited data, applying data augmentation techniques (like random crops, flips, rotations, etc.) is one of the most effective ways to improve model generalization. This artificially expands your dataset by creating variations of the existing images. Using a very large model with many parameters would likely lead to overfitting on a small dataset. Removing regularization would also lead to overfitting, and using a very high learning rate would likely cause training instability."
        },
        "quiz_4": {
            "title": "Pooling Layers in CNNs",
            "question": "What is the primary purpose of pooling layers in CNNs?",
            "options": [
                "To introduce non-linearity into the network",
                "To extract features from the input image",
                "To reduce spatial dimensions and provide translation invariance",
                "To normalize pixel values across all feature maps"
            ],
            "correct": 2,
            "explanation": "The primary purpose of pooling layers is to reduce the spatial dimensions of the feature maps and provide some degree of translation invariance. By downsampling the feature maps, pooling helps make the network robust to small variations in the position of features. Non-linearity is introduced by activation functions (like ReLU), feature extraction is done by convolutional layers, and normalization is typically handled by batch normalization layers."
        },
        "quiz_5": {
            "title": "Kernel Size Selection",
            "question": "When should you consider using larger kernel sizes (e.g., 5×5 or 7×7) instead of 3×3 kernels in a CNN?",
            "options": [
                "When you need to reduce computation time",
                "When processing high-resolution images and need to capture larger spatial patterns",
                "When your dataset has very few samples",
                "When training on a CPU instead of a GPU"
            ],
            "correct": 1,
            "explanation": "Larger kernel sizes are beneficial when you need to capture larger spatial patterns in high-resolution images. They increase the receptive field size more rapidly, allowing the network to 'see' more of the input image at once. However, they also increase computational cost (not reduce it), require more training data (not less), and are more computationally intensive on any hardware platform including CPUs."
        }
    }
}